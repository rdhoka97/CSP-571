Name:- PARTH RATHOD
CWID:- A20458817
HW :- HW5
COURSE:- DPA CS571
Recitation
Question 1
a)
```{r}
knitr::include_graphics("1a-1.jpeg")
knitr::include_graphics("1a-2.jpeg")
```
b)
In K-means clustering algorithm, at each iteration, an observation is assigned to its nearest cluster. Due to which
after each iteration the value of RHS will decreases as this quantity is sum of squared distance of each observation
from the cluster mean. Hence, in this way the k-means will decrease the objective in each iteration.
Question 2
a)
```{r}
knitr::include_graphics("2a.jpeg")
```
```{r}
dend = as.dist(matrix(c(0, 0.3, 0.4, 0.7,
0.3, 0, 0.5, 0.8,
0.4, 0.5, 0.0, 0.45,
0.7, 0.8, 0.45, 0.0), nrow = 4))
plot(hclust(dend, method = "complete"))
dend = as.dist(matrix(c(0, 0.3, 0.4, 0.7,
0.3, 0, 0.5, 0.8,
0.4, 0.5, 0.0, 0.45,
0.7, 0.8, 0.45, 0.0), nrow = 4))
plot(hclust(dend, method = "complete"))
b)
```{r}
knitr::include_graphics("2b.jpeg")
```
```{r}
plot(hclust(dend, method = "single"))
plot(hclust(dend, method = "single"))
c)
In this case, we have clusters (1,2) and (3,4).
d)
In this case, we have clusters ((1,2),3) and (4).
e)
```{r}
plot(hclust(dend, method = "complete"), labels = c(2,1,4,3))
```
Question 3
a)
```{r}
x <- cbind(c(1, 1, 0, 5, 6, 4), c(4, 3, 4, 1, 2, 0))
plot(x[,1], x[,2])
```
b)
```{r}
set.seed(1)
labels <- sample(2, nrow(x), replace = T)
labels
c)
```{r}
centroid1 <- c(mean(x[labels == 1, 1]), mean(x[labels == 1, 2]))
centroid2 <- c(mean(x[labels == 2, 1]), mean(x[labels == 2, 2]))
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
d)
```{r}
labels <- c(1, 1, 1, 2, 2, 2)
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
```
e)
```{r}
centroid1 <- c(mean(x[labels == 1, 1]), mean(x[labels == 1, 2]))
centroid2 <- c(mean(x[labels == 2, 1]), mean(x[labels == 2, 2]))
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
```
If we assign each observation to the centroid to which it is closest, nothing changes, so the algorithm is terminated at this step.
f)
```{r}
plot(x[, 1], x[, 2], col=(labels + 1), pch = 20, cex = 2)
```
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
Question 4
a)
There is not enough information to tell. For example, if d(1,4)=2, d(1,5)=3, d(2,4)=1, d(2,5)=3, d(3,4)=4 and d(3,5)=1, the single linkage dissimilarity between {1,2,3} and {4,5} would be equal to 1 and the complete linkage dissimilarity between {1,2,3} and {4,5} would be equal to 4. So, with single linkage, they would fuse at a height of 1, and with complete linkage, they would fuse at a height of 4. But, if all inter-observations distance are equal to 2, we would have that the single and complete linkage dissimilarities between {1,2,3} and {4,5} are equal to 2.
b)
They would fuse at the same height. For example, if d(5,6)=2, the single and complete linkage dissimilarities between {5} and {6} would be equal to 2. So, they would fuse at a height of 2 for single and complete linkage.
Practicum Problems
Problem 1
From the above mean and variance values it is clear that values are on different scale. So, we need to perform
scaling before applying PCA to our dataset.
From the above plot we can see that feature malic_acid is pointed in opposite direction to the feature hue.
#check the variance of the predictors
```{r}
```{r}
#calculating proportion of variance for each principle component
library(corrplot)
