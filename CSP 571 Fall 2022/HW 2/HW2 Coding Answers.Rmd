---
output:
  html_document: default
  pdf_document: default
---

#Problem 1
```{r}
library(readr)
library(data.table)
library(corrplot)
library(caret)
library(ROCR)
```

```{r}
abaloneURl = "https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data"
abaloneData = fread(abaloneURl, header = FALSE) 
abaloneHeader = c("Sex","Length","Diameter","Height","Whole weight","Shucked weight","Viscera weight","Shell weight","Rings")
colnames(abaloneData) = abaloneHeader
#Display Summary:
summary(abaloneData)
```


```{r}
#Returns the indices when the following condition is true in which()
condition = which(abaloneData$Sex!="I")
#All the Rows with Sex = I are removed
abaloneData2 = abaloneData[condition]
#check for all sex categories in new dataset:
unique(abaloneData2$Sex)
#As the response variable need to be numeric, converting categorical Sex to a factor
abaloneData2$Sex = factor(abaloneData2$Sex)
#check for the change:
str(abaloneData2)
```
```{r}
#Creating 80-20 Training Testing Split, createDataPartition() returns the indices
trainIndex = createDataPartition(y = abaloneData2$Sex, p = 0.8, list = FALSE)
#Training data
trainData = abaloneData2[trainIndex,]
#Testing data (note the minus sign)
testData = abaloneData2[-trainIndex,]

```


```{r}
#Predicting Sex using glm
model <- glm(Sex~.,family=binomial,data=trainData)
#Summary of our model
summary(model)
#Coefficients of our model
coef(model)
#Confidence Interval
confint(model)


```
From the above observations for confidence interval, we can see that, all the predictors, except the “Shucked Weight” contain 0 within their confidence interval range which is in line with the basic assumption of Null Hypothesis which says, No relationship between X and Y.
The only attribute with a substantially low p-value is “Shucked Weight” with a p-value of 0.00105 and all the other attributes have a high p-value. Thus, we can conclude that, there is no relationship between the predictors and the response variable (except the “Shucked Weight”) and Null Hypothesis holds true for all the predictors except the “Shucked Weight”. Hence Shucked Weight is the only predictor which is relevant.

```{r}
#Predict [By setting the parameter type='response', R will output probabilities in the form of P(y=1|X)]
probs = predict(model, testData, type = "response")
#Using a 50% cut-off factor i.e probabilities > 0.5 are Males and rest are Females 
resultSet = ifelse(probs > 0.5,"M","F")
resultSet2 = factor(resultSet)
#Creating a confusion matrix
confusionMatrix(resultSet2, testData$Sex)
```

```{r}
#Plotting the ROC Curve
roc.pred = prediction(probs,testData$Sex)
roc.perf = performance(roc.pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf)
abline(0,1)
auc.perf = performance(roc.pred, measure = "auc")
cat("Area Under the Curve: ")
auc.perf@y.values
#Plotting the correlations between the predictors
cm = cor(abaloneData2[,-1])
corrplot(cm, method = "number")

```

The above figure clearly shows that there is a positive linear connection for all factors. Only the Rings predictor has a weak uphill (positive) association, whereas the others have a high uphill (positive) relationship.




#Problem 2


```{r}

library(data.table) #Data Import
library(e1071)      #Naive Bayes


```

```{r}
#Setting up the URL for data import
mushroomURL = "https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data"
mushroomData = fread(mushroomURL,header=FALSE)
#Adding the headers
mushroomHeader = c("Class","cap-shape","cap-surface","cap-color","bruises","odor","gill-attachment","gill-spacing","gill-size","gill-color","stalk-shape","stalk-root","stalk-surface-above-ring","stalk-surface-below-ring","stalk-color-above-ring","stalk-color-below-ring","veil-type","veil-color","ring-number","ring-type","spore-print-color","population","habitat")
colnames(mushroomData) = mushroomHeader
#Display Summary:
summary(mushroomData)

```

```{r}

#Class Distribution
table(mushroomData$Class)
#Converting Class Attribute to a factor
mushroomData$Class = factor(mushroomData$Class)
#Dimensions
dim(mushroomData)
#Structure 
str(mushroomData)
```

```{r}
#Finding the number of missing values
cat("Number of missing values = ",sum(mushroomData=="?"))
#New dataset with removed missing values
mushroomData2 = mushroomData[mushroomData$`stalk-root`!="?"]

```
There is no damage in removing the observations with missing values because we have plenty after omitting the ones with missing values.


```{r}
#Creating a split
trainSize = floor(0.80*nrow(mushroomData2))
trainIndex = sample(nrow(mushroomData2), size = trainSize)
trainData = mushroomData2[trainIndex,]
testData = mushroomData2[-trainIndex,]
```


```{r}
#Naive Bayes 
model = naiveBayes(trainData[,-1],trainData$Class)
#Prediction on Testing Data
testPred = predict(model,testData[,-1])
#Prediction on Training Data
trainPred = predict(model,trainData[,-1])
#Accuracy of Testing Model
cat("Accuracy of Testing Model: ",mean(testPred == testData$Class)*100,"%")
#Accuracy of Training Model
cat("Accuracy of Training Model: ",mean(trainPred == trainData$Class)*100,"%")
#Confusion Matrix
table(testPred, testData$Class)
```
From the above the confusion matrix we have the following:
TP = 693, FP = 60, FN = 4 and TN = 372 



#Problem 3

```{r}
library(readr)
library(data.table)
library(caret)
```

```{r}
hd_URL = "http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data"
hd_data = fread(hd_URL, header = FALSE)

hd_Header = c("longitudinalPos","prismaticCoef","LDR","BDR","LBR","froudeNo","Residuary")
colnames(hd_data) = hd_Header

#Direct use of attribute names in code
attach(hd_data)
```

```{r}
#Creating 80-20 Training Testing Split, createDataPartition() returns the indices
trainIndex = createDataPartition(y = hd_data$Residuary , p = 0.8, list = FALSE)

#Training data
trainData = hd_data[trainIndex,]

#Testing data (note the minus sign)
testData = hd_data[-trainIndex,] 
```

```{r}
#Training fit for linear model
linearModel1 = lm(hd_data$Residuary~hd_data$longitudinalPos + hd_data$prismaticCoef + hd_data$LDR + hd_data$BDR + hd_data$BDR + hd_data$LBR +hd_data$froudeNo, data = trainData)

```

```{r}
# Function to compute MSE
MSE = function(yActual, yPred)
{
  return (mean((yActual - yPred)^2))
}
```

```{r}
mse1 = MSE(hd_data$Residuary, linearModel1$fitted.values )

# Summarize the results
cat("Training MSE: ", mse1)
cat("Training RMSE: ", sqrt(mse1))
cat("Training R-squared: ",summary(linearModel1)$r.sq)
```

```{r}
# Define training control
train.control =  trainControl(method = "boot", number = 1000)

# Train the model
linearModel2 = train(Residuary~., data = trainData, method = "lm", trControl = train.control)

```


```{r}
# 5 Point Summary for resulting RMSE for each resample  
summary(linearModel2$resample$RMSE)

# 5 Point Summary for resulting R-Squared for each resample 
summary(linearModel2$resample$Rsquared)
```


```{r}
# Histogram of RMSE values
hist(linearModel2$resample$RMSE, xlab = "RMSE Values", main = "Histogram of the RMSE values")
```

```{r}
# Calculating the MSE from RMSE
mse2 = mean(linearModel2$resample$RMSE)^2

# Summarize the results
cat("Training Mean MSE (Bootstrap): ", mse2)
cat("Training Mean RMSE (Bootstrap): ", mean(linearModel2$resample$RMSE))
cat("Training Mean R-squared (Bootstrap): ",mean(linearModel2$resample$Rsquared))
```

```{r}
predVals_boot = predict(linearModel2,testData)
mse3 = MSE(testData$Residuary, predVals_boot)
```


```{r}
RSS = function (yActual, yPred)
{
  return (sum((yActual - yPred)^2))
}

TSS = function (yActual)
{
  return (sum((yActual - mean(yActual))^2))
}
```


```{r}
rss = RSS(testData$Residuary, predVals_boot)
tss = TSS(testData$Residuary)

# Summarize the results
cat("Testing MSE (Bootstrap): ", mse3)
cat("Testing RMSE (Bootstrap): ", sqrt(mse3))
cat("Testing Mean R-squared (Bootstrap): ",1 - (rss/tss))
```
On the test set, there is no difference in performance between the original and bootstrap models.


#Problem 4

```{r}
library(readr)
library(data.table)
library(caret)
```

```{r}
gcd_URL = "https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data-numeric"
gc_data = fread(gcd_URL, header = FALSE)
```

```{r}
#As the response variable need to be numeric, converting categorical V25 to a factor
gc_data$V25 = factor(gc_data$V25)
```

```{r}
#Creating 80-20 Training Testing Split, createDataPartition() returns the indices
trainIndex = createDataPartition(y = gc_data$V25 , p = 0.8, list = FALSE)

#Training data
trainData = gc_data[trainIndex,]

#Testing data (note the minus sign)
testData = gc_data[-trainIndex,] 
```


```{r}
# Creating model for y = V25 using glm
logisticModel1 = glm(V25~.,family=binomial,data=trainData)
```


```{r}
actualVals = trainData$V25

# Using a 50% cut-off factor i.e probabilities > 0.5 are 2 and rest are 1 
fittedVals = ifelse(logisticModel1$fitted.values > 0.5,2,1)
fittedVals = factor(fittedVals)
```

```{r}
# Confusion matrix
cm = confusionMatrix(fittedVals, trainData$V25)
cm
```

```{r}
# Summarize the results
cat("Training Precision: ", cm$byClass[5] * 100, "%")
cat("Training Recall: ", cm$byClass[6] * 100, "%")
cat("Training F1-Score: ", cm$byClass[7] * 100, "%")
```

```{r}
# Predict [By setting the parameter type='response', R will output probabilities in the form of P(y=1|X)]
probs = predict(logisticModel1, testData, type = "response")

#Using a 50% cut-off factor i.e probabilities > 0.5 are Males and rest are Females 
fittedVals_test = ifelse(probs > 0.5,2,1)
fittedVals_test = factor(fittedVals_test)

# Confusion matrix
cm_test = confusionMatrix(fittedVals_test, testData$V25)
cm_test
```

```{r}
# Summarize the results
cat("Testing Precision: ", cm_test$byClass[5] * 100, "%")
cat("Testing Recall: ", cm_test$byClass[6] * 100, "%")
cat("Testing F1-Score: ", cm_test$byClass[7] * 100, "%")
```


#--------------------Cross-Validation---------------------------
```{r}
# Define training control
train.control =  trainControl(method = "cv", number = 10)

# Train the model
logisticModel2 = train(V25~., data = trainData, method = "glm", family = "binomial", trControl = train.control)

```


```{r}
fittedVals_cv = ifelse(logisticModel2$finalModel$fitted.values > 0.5,2,1)
fittedVals_cv = factor(fittedVals_cv)

# Confusion matrix
cm_cv = confusionMatrix(fittedVals_cv, trainData$V25)
cm_cv

# Summarize the results
cat("Training Precision with 10-fold CV: ", cm_cv$byClass[5] * 100, "%")
cat("Training Recall with 10-fold CV: ", cm_cv$byClass[6] * 100, "%")
cat("Training F1-Score with 10-fold CV: ", cm_cv$byClass[7] * 100, "%")
```

```{r}
# Predict [By setting the parameter type='response', R will output probabilities in the form of P(y=1|X)]
probs_cv = predict(logisticModel2, testData, type = "prob")

#Using a 50% cut-off factor i.e probabilities > 0.5 are Males and rest are Females 
fittedVals_cv_test = ifelse(probs > 0.5,2,1)
fittedVals_cv_test = factor(fittedVals_test)

# Confusion matrix
cm_cv_test = confusionMatrix(fittedVals_test, testData$V25)
cm_cv_test

# Summarize the results
cat("Testing Precision: ", cm_cv_test$byClass[5] * 100, "%")
cat("Testing Recall: ", cm_cv_test$byClass[6] * 100, "%")
cat("Testing F1-Score: ", cm_cv_test$byClass[7] * 100, "%")
```

On the test set, there is no difference in performance between the original and bootstrap models.




