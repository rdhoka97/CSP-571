Chapter 8
Question 3

```{r}
p1 = seq(0 + 1e-06, 1 - 1e-06, length.out = 100)
p2 = 1 - p1
class_error = 1 - apply(rbind(p1, p2), 2, max)
gini_index = p1 * (1 - p1) + p2 * (1 - p2)
cross_entropy = -(p1 * log(p1) + p2 * log(p2))
plot(p1, class_error, type = "l", col = "black", xlab = "Pm1", ylab = "Error
metrics", ylim = c(min(c(class_error, gini_index, cross_entropy)), max(class_error,
gini_index, cross_entropy)))
lines(p1, gini_index, col = "blue")
lines(p1, cross_entropy, col = "red")
legend(0.3, 0.2, c("Classification error", "Gini index", "Cross entropy"), col =
c("black", "blue", "red"), lty = c(1, 1))
grid()
```
Question 5

We have 2 classes Red and Green
Different Approach to combine Results are:-

1) Majority vote Approach:-
Out of 10 estimates we can see that 6 estimates have p>0.5 and 4 estimates with p<0.5, which suggest that majority of estimates classify X as Red.

2)Average Probability Approach:-
Here we calculate average of all estimates and depends on the result if p>0.5, then class for X will be Red otherwise Green
The average of 10 estimates comes out to be 0.45
Since p<0.5, Class for X will be Green.



Chapter 9

Question 1
```{r}

x1 <- -10:10
x2 <- 1 + 3 * x1
plot(x1, x2, type = "l", col = "blue")
text(c(0), c(-20), "Greater than 0", col = "blue")
text(c(0), c(20), "Less than 0", col = "blue")
lines(x1, 1 - x1/2, col = "red")
text(c(0), c(-15), "Less than 0", col = "red")
text(c(0), c(15), "Greater than 0", col = "red")
```


Question 2

a)
```{r}
plot(NA, NA, type = "n", xlim = c(-4, 2), ylim = c(-1, 5), asp = 1, xlab = "X1", ylab = "X2")
symbols(c(-1), c(2), circles = c(2), add = TRUE, inches = FALSE)
```

b)
```{r}
plot(NA, NA, type = "n", xlim = c(-4, 2), ylim = c(-1, 5), asp = 1, xlab = "X1", ylab = "X2")
symbols(c(-1), c(2), circles = c(2), add = TRUE, inches = FALSE)
text(c(-1), c(2), "< 4")
text(c(-4), c(2), "> 4")
```

c)
```{r}
plot(c(0, -1, 2, 3), c(0, 1, 2, 8), col = c("blue", "red", "blue", "blue"), 
    type = "p", asp = 1, xlab = "X1", ylab = "X2")
symbols(c(-1), c(2), circles = c(2), add = TRUE, inches = FALSE)
```
It sufficient to replace X1 and X2 by the coordinates of the points in the equation and to check if the result is less or greater than 4. 
For (0,0), we have 5>4 (blue class) 
For (−1,1), we have 1<4 (red class)
For (2,2), we have 9>4 (blue class)
For (3,8), we have 52>4 (blue class).


d)
It is obvious when we expand the equation of decision boundary it becomes
X1^2 + X2^2 + 2X1 - 4X2 + 1 = 0
which is linear.


Question 3
a)
```{r}
x1 = c(3, 2, 4, 1, 2, 4, 4)
x2 = c(4, 2, 4, 4, 1, 3, 1)
colors = c("red", "red", "red", "red", "blue", "blue", "blue")
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
```

b)
The optimal separating hyperplane has to be between the observations (2,1) and (2,2), and between the observations (4,3) and (4,4). 
So it is a line that passes through the points (2,1.5) and (4,3.5) with equation

X1 − X2 − 0.5 = 0


```{r}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
```

c)
The classification rule is 
Classify to Red if X1 − X2 − 0.5 < 0, and 
Classify to Blue otherwise.

d)
```{r}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
abline(-1, 1, lty = 2)
abline(0, 1, lty = 2)
```
The margin is here equal to 1/4

e)
The support vectors are the points (2,1), (2,2), (4,3) and (4,4)

f)
By examining the plot, it is clear that if we moved the observation (4,1), we would not change the maximal margin hyperplane as it is not a support vector.

g)
For example, the hyperplane which equation is X1−X2−0.3=0 is not the optimal separating hyperplane.
```{r}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.3, 1)
```

h)
```{r}
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
points(c(3), c(1), col = c("red"))
```
When the red point (3,1) is added to the plot, the two classes are obviously not separable by a hyperplane anymore








Practicum Problems

Question 1
```{r}
library(rpart)
library(rpart.plot)
```

Function Definition
```{r}
gini <- function(p) 
{
  gini.index = 2 * p * (1 - p)
  return (gini.index)
}

entropy <- function(p) 
{
  entropy = (p * log(p) + (1 - p) * log(1 - p))
  return (entropy)
}


```


```{r}
set.seed(150)
a<-rnorm(n=150,mean=5,sd=2)
b<-rnorm(n=150,mean=-5,sd=2)
data1 <- data.frame(val = a,label=rep("y",150))
data2 <- data.frame(val = b,label=rep("n",150))
data <- rbind(data1,data2)
data$label <- as.factor(data$label)
d_tree <- rpart(label~val,data,method="class")
rpart.plot(d_tree)
```
From the above we can see that threshold value for the first split will be -0.06. The tree has one root
node and two leaf nodes. Also, tree is able to classify both classes separately which clearly shows empirical
distribution.


Calculating Gini and Entropy for Each Node:
p=probability of each node
```{r}

p=c(.5, 0, 1)

gini_values=sapply(p, gini)
gini_values
entropy_values=sapply(p, entropy)
entropy_values
```
The gini values for above tree will be 0.5, 0.0, 0.0
The entropy values for above tree will be -0.6931472, NaN, NaN


```{r}
set.seed(150)
a1<-rnorm(n=150,mean=1,sd=2)
b1<-rnorm(n=150,mean=-1,sd=2)
data3 <- data.frame(val = a1,label=rep("y",150))
data4 <- data.frame(val = b1,label=rep("n",150))
dataa <- rbind(data3,data4)
dataa$label <- as.factor(dataa$label)
d_tree1 <- rpart(label~val,dataa,method="class")
rpart.plot(d_tree1)
```
From the above tree we can see that threshold value for the first split is 0.36. The tree has total of 13
nodes in which one of the nodes is root node and has total of 7 leaf nodes. Large tree size shows presence of
more different labels in node, which resulted in a large tree. So, this tree has more overlapping of labels in
nodes




Calculating Gini and Entropy for Each Node:
p=probability of each node
```{r}

p1=c(.5,0.22,0.72,0.28,0.53,0.45,0.09,0.23,0.70,0.37,0.59,1.0,0.81)
gini_values1=sapply(p1, gini)
gini_values1
entropy_values1=sapply(p1, entropy)
entropy_values1
```
The gini values for above tree will be 0.5000, 0.3432, 0.4032, 0.4032, 0.4982, 0.4950, 0.1638, 0.3542, 0.4200,
0.4662,0.4838, 0.0000, 0.3078

The entropy values for above tree will be -0.6931472, -0.5269080, -0.5929533, -0.5929533, -0.6913461, -0.6881388, -
0.3025378, -0.5392763, -0.6108643, -0.6589557, -0.6768585, NaN, -0.4862230




```{r}
new_tree <- prune.rpart(d_tree1,cp=0.1)
rpart.plot(new_tree)
```
From the above tree we can see that threshold value for the first split will be 1.5. The tree has one root node
and 2 leaf nodes. Also, this pruned tree is much better than the previous as this has only two leaf nodes with
less overlapping labels.



Calculating Gini and Entropy for Each Node:
p=probability of each node
```{r}

p2=c(.5,0.22,0.72)
gini_values2=sapply(p2, gini)
gini_values2
entropy_values2=sapply(p2, entropy)
entropy_values2
```
The gini values for above tree will be 0.5000, 0.3432, 0.4032
The entropy values for above tree will be -0.6931472, -0.5269080, -0.5929533







Problem 2

Import Libraries
```{r}
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)

```


```{r}
white.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
white.raw <- read.csv(white.url, header = TRUE, sep = ";")
white <- white.raw
str(white)


red.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
red.raw <- read.csv(red.url, header = TRUE, sep = ";")
red <- red.raw
str(red)
```
The output tells us that there are 4898 samples and 12 variables in WHITE WINE Dataset.
The output tells us that there are 1599 samples and 12 variables in RED WINE Dataset.



Train Test Split
```{r}
white$quality <- as.factor(white$quality)
inTrain <- createDataPartition(white$quality, p = 0.8, list = F)
train.white <- white[inTrain,]
test.white <- white[-inTrain,]

red$quality <- as.factor(red$quality)
inTrainr <- createDataPartition(red$quality, p = 0.8, list = F)
train.red <- red[inTrainr,]
test.red <- red[-inTrainr,]
```



Decision Tree Model
```{r}
dt.train.white <- rpart(quality~., data=train.white )
rpart.plot(dt.train.white)

dt.train.red <- rpart(quality~., data=train.red )
rpart.plot(dt.train.red)
```

Displaying Confusion Matrix
```{r}

print("Confusion matrix for White Wine Dataset")
dt.predict.white <- predict(dt.train.white, test.white, type = 'class')
confusionMatrix(dt.predict.white, test.white$quality)

print("-------------------------------------------------------------------------")
print("-------------------------------------------------------------------------")
print("-------------------------------------------------------------------------")

print("Confusion matrix for Red Wine Dataset")
dt.predict.red <- predict(dt.train.red, test.red, type = 'class')
confusionMatrix(dt.predict.red, test.red$quality)
```
Decision Tree returned an accuracy of 52.5% (+-2) for White Wine Dataset
Decision Tree returned an accuracy of 53.9% (+-2) for  Red Wine Dataset

For White Wine Dataset the first split was done at "alcohol < 11" 
whereas In Red Wine Dataset the first split was done at "alcohol < 9.5"

Sulphates was taken into consideration in Red Wine Dataset 
whereas its absent in White Wine Dataset.

Total Sulfur Dioxide was taken into consideration in Red Wine Dataset 
whereas its absent in White Wine Dataset.

Free Sulfur Dioxide was taken into consideration in White Wine Dataset 
whereas its absent in Red Wine Dataset.



Random Forest Model
```{r}
rf.train.white <- train(quality ~ ., data = train.white, method = "rf",preProcess = c("center", "scale"))

rf.train.red <- train(quality ~ ., data = train.red, method = "rf",preProcess = c("center", "scale"))
```
Displaying the Confusion Matrix
```{r}
print("Confusion matrix for White Wine Dataset")
rf.predict.white <- predict(rf.train.white, test.white)
confusionMatrix(rf.predict.white, test.white$quality)

print("-------------------------------------------------------------------------")
print("-------------------------------------------------------------------------")
print("-------------------------------------------------------------------------")

print("Confusion matrix for Red Wine Dataset")
rf.predict.red <- predict(rf.train.red, test.red)
confusionMatrix(rf.predict.red, test.red$quality)
```

Random Forest returned an accuracy of 69.4% (+-2) for White Wine Dataset
Random Forest returned an accuracy of 71.9% (+-2) for Red Wine Dataset


The Accuracy increased from 52% to 69% in Random Forest Classifier in White Wine Dataset
The Accuracy increased from 53% to 71% in Random Forest Classifier in Red Wine Dataset




Problem 3

Importing Libraries
```{r}
library(readxl)
library(tm)
library(SnowballC)
library(e1071)
```

```{r}
smsData <- read_excel("D:\\Temp\\sms_spam.xlsx")
str(smsData)
```

```{r}
names(smsData)[1] = "type"
names(smsData)[2] = "text"
```

```{r}
smsData$type <- factor(smsData$type)
str(smsData)
```
```{r}
table(smsData$type)
```
```{r}
smsDataCorpus <- VCorpus(VectorSource(smsData$text))
print(smsDataCorpus)
```
Stemming
```{r}
smsData_corpus_clean <- tm_map(smsDataCorpus,stemDocument)
```


Convert to LowerCase
```{r}
smsData_corpus_clean <- tm_map(smsData_corpus_clean,content_transformer(tolower))
as.character(smsData_corpus_clean[[1]])
```

Remove Stop Words
```{r}
smsData_corpus_clean <- tm_map(smsData_corpus_clean,removeWords,stopwords())
```

Remove Punctuation
```{r}
smsData_corpus_clean <- tm_map(smsData_corpus_clean,removePunctuation)
```

Remove WhietSpace
```{r}
smsData_corpus_clean <- tm_map(smsData_corpus_clean,stripWhitespace)
```

```{r}
as.character(smsData_corpus_clean[[1]])
```

Creating Document Term Matrix
```{r}
sms_dtm <- DocumentTermMatrix(smsData_corpus_clean)
sms_dtm
```

Splitting into Train Test Split in 75% Training and 25% into Testing
```{r}
sms_dtm_train <- sms_dtm[1:4169,]
sms_dtm_test <- sms_dtm[4170:5571,]


sms_train_labels <- smsData[1:4169,]$type
sms_test_labels <- smsData[4170:5571,]$type
```

Using FindFrequencyTerms
```{r}
frequent_terms <- findFreqTerms(sms_dtm_train,10)

sms_dtm_freq_train <- sms_dtm_train[,frequent_terms]
sms_dtm_freq_test <- sms_dtm_test[,frequent_terms]

```

Function to Convert into Booleans
```{r}
convert_counts <- function(x){
  x <- ifelse(x > 0,1,0)
}
```

```{r}
sms_train <- apply(sms_dtm_freq_train,MARGIN = 2,convert_counts)
sms_test <- apply(sms_dtm_freq_test,MARGIN = 2,convert_counts)
```

Building the Model
```{r}

sms_classifier <- naiveBayes(sms_train,sms_train_labels)

#Predicting using the Model
sms_train_pred <- predict(sms_classifier,sms_train)
sms_test_pred <- predict(sms_classifier,sms_test)

```


Displaying the Accuracy
```{r}

print("Train Set Accuracy with Confusion Matrix:")
confusionMatrix(sms_train_pred, sms_train_labels)


print("Test Set Accuracy with Confusion Matrix:")
confusionMatrix(sms_test_pred, sms_test_labels)

```
Training Set Accuracy is 14&
Test Set Accuracy is 13%








